{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d658c-5acf-439c-bb39-e62bb2d868b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier # 결정트리 분류모델\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#####################################################################\n",
    "# 웹 크롤링 (Web Crawling) - BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹 페이지 가져오기\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# BeautifulSoup으로 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# 필요한 데이터 추출\n",
    "data = []\n",
    "table = soup.find(\"table\", {\"class\": \"data\"})\n",
    "rows = table.find_all(\"tr\")\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(data, columns=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "print(df)\n",
    "\n",
    "# 데이터 시각화 (예: Column1의 분포)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Column1'], kde=True)\n",
    "plt.title('Distribution of Column1')\n",
    "plt.xlabel('Column1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# konlpy\n",
    "from konlpy.tag import Kkma #형태소분석기\n",
    "import pandas as pd\n",
    "\n",
    "kkma = Kkma() #형태소분석기 kkma 정의\n",
    "\n",
    "df = pd.read_excel(\"result.xlsx\")\n",
    "df['SUM'] = df['제목'] + \" \" + df['내용']\n",
    "total = []\n",
    "\n",
    "for i in df['SUM']:\n",
    "    nouns = kkma.nouns(i)\n",
    "    total+=nouns\n",
    "    \n",
    "from collections import Counter \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "dic = Counter(total) # 제목/내용에 있는 명사(total)를 빈도를 센 것을 dic에 넣음\n",
    "\n",
    "image = Image.open(\"heart.png\") # 하트 이미지를 가져옴\n",
    "image2 = np.array(image) #이미지를 행렬로 바꿔라\n",
    "\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "              font_path=\"BMDOHYEON_otf.otf\",\n",
    "              colormap=\"Blues\",\n",
    "              mask = image2)\\\n",
    "            .generate_from_frequencies(dic)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(wc)\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# 머신러닝 (Machine Learning) - 기본 프로세스\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# 데이터 확인 및 전처리\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "data = data.dropna()\n",
    "\n",
    "# 특성과 타겟 분리\n",
    "X = data.drop('target_column', axis=1).values\n",
    "y = data['target_column'].values\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 특성 상관 관계 히트맵\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 타겟 분포\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='target_column', data=data)\n",
    "plt.title('Target Distribution')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# Logistic Regression -- Scaler 사용\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC 곡선\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# K-최근접 이웃 (KNN) -- Scaler 사용\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_test_scaled, y_test)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy}\")\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(X_train, y_train) # 스케일링 필요없음\n",
    "dtc.score(X_test, y_test)\n",
    "y_pred = dtc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Classifier Accuracy: {accuracy}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dtc, filled=True, feature_names=data.columns[:-1], class_names=['Class 0', 'Class 1'])\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# 랜덤 포레스트 (Random Forest)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True)\n",
    "rf.fit(X_train, y_train) # 스케일링 필요없음\n",
    "rf.oob_score_\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy}\")\n",
    "\n",
    "# 중요 특징 시각화\n",
    "feature_importances = rf.feature_importances_\n",
    "features = data.columns[:-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances in Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# KNeighborsRegressor -- Scaler 사용\n",
    "knr = KNeighborsRegressor(n_neighbors=3)\n",
    "knr.fit(X_train_scaled, y_train)\n",
    "knr.score(X_test_scaled, y_test)\n",
    "y_pred = knr.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"KNN Regressor Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
    "rfr.fit(X_train, y_train) # 스케일링 필요없음\n",
    "rfr.oob_score_\n",
    "y_pred = rfr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Random Forest Regressor Mean Squared Error: {mse}\")\n",
    "\n",
    "# 중요 특징 시각화\n",
    "feature_importances = rfr.feature_importances_\n",
    "features = data.columns[:-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances in Random Forest Regressor Model')\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# 딥러닝 (Deep Learning) - 기본 프로세스\n",
    "\n",
    "# 데이터 로드 및 전처리 (여기서는 MNIST 예제)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "# 이진 분류모델 : output activation-sigmoid\n",
    "# 다중 분류모델 : output activation-softmax\n",
    "# 회귀모델 : output activation- 없음 또는 relu\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 학습 과정 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# CNN (Convolutional Neural Networks)\n",
    "\n",
    "# 데이터 로드 및 전처리 (여기서는 MNIST 예제)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 학습 과정 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# XGBoost - 기본 프로세스\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data = pd.read_csv('pima_indians.csv')\n",
    "X = data.drop('target_column', axis=1).values\n",
    "y = data['target_column'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DMatrix 생성\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, 'test')], early_stopping_rounds=10)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = bst.predict(dtest)\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "print(f\"XGBoost Accuracy: {accuracy}\")\n",
    "\n",
    "# 중요 특징 시각화\n",
    "plot_importance(bst)\n",
    "plt.show()\n",
    "\n",
    "# GridSearchCV 설정 및 실행\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "                           param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy found: {grid_search.best_score_}\")\n",
    "\n",
    "# 최적 모델로 재학습 및 평가\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Best XGBoost Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad3426-1534-44aa-8983-d88a9d1d5aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
